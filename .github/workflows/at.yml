name: Train AI on New Data

on:
  push:
    paths:
      - 'Training_Data/**'
  workflow_dispatch:

jobs:
  validate-train:
    name: Validate and Train AI via SageMaker
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Dependencies
        run: |
          pip install -r requirements.txt
          pip install awscli boto3 huggingface-hub

      - name: Download and Prepare Stock Data
        run: |
          python download_and_prepare_stock_data.py

      - name: Format for ALBERT
        run: |
          python format_for_albert.py --folder Training_Data

      - name: Validate and Prepare Data
        run: |
          python validate_and_prepare.py

      - name: Optimize Data for AI Model
        run: |
          python optimize_data.py --input Training_Data/validated_data.csv --output Training_Data/optimized_data.csv

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Upload Optimized Data to S3
        run: |
          aws s3 cp Training_Data/optimized_data.csv s3://${{ secrets.S3_BUCKET }}/training/optimized_data.csv

      - name: Launch SageMaker Training Job
        env:
          SAGEMAKER_ROLE_ARN: ${{ secrets.SAGEMAKER_ROLE_ARN }}
          S3_BUCKET: ${{ secrets.S3_BUCKET }}
        run: |
          python launch_sagemaker_training.py \
            --role-arn "$SAGEMAKER_ROLE_ARN" \
            --bucket "$S3_BUCKET" \
            --input-key "training/optimized_data.csv" \
            --output-key "model/model.tar.gz"

      - name: Download Trained Model from S3
        run: |
          mkdir -p trained_model
          aws s3 cp s3://${{ secrets.S3_BUCKET }}/model/model.tar.gz trained_model/model.tar.gz
          tar -xzf trained_model/model.tar.gz -C trained_model

      - name: Upload model weights to Hugging Face
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          huggingface-cli login --token $HF_TOKEN
          huggingface-cli upload rmanzo28/Stockai trained_model/model_weights.pth
